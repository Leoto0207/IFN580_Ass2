{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7fc7f33-94fe-4ab8-94ff-27347c335568",
   "metadata": {},
   "source": [
    "# Prepare SQuAD_tiny Dataset for Assignment 2\n",
    "\n",
    "This code prepare SQuAD_tiny from the SQuAD dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00cc501e-66e7-4dd8-9ede-7dd516c57230",
   "metadata": {},
   "source": [
    "# 0. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "debe47fc-6d7a-4fcb-b07f-7435895e88ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer,AutoTokenizer, TrainingArguments, DataCollatorForSeq2Seq, EarlyStoppingCallback, AutoModelForSeq2SeqLM,TrainerCallback\n",
    "from rouge_score import rouge_scorer\n",
    "from tqdm import tqdm\n",
    "from transformers import logging as transformers_logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28c25606-856c-4e9b-bbb3-86d0abdd798b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x139175a10>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set seed for reproducibility\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f9a46e-5f00-41af-9148-a6eefb6d08ed",
   "metadata": {},
   "source": [
    "# 1. Load and preprocess SQuAD dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0437040-ad9b-4cfe-9801-d1a801a7102e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load and preprocess SQuAD dataset\n",
    "dataset = load_dataset(\"squad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42bd720f-173b-47ea-9c85-abe55b0ae8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take subsets to avoid overload\n",
    "train_dataset = dataset[\"train\"].select(range(10000,11000))\n",
    "val_dataset = dataset[\"validation\"].select(range(3000,3100))\n",
    "test_dataset = dataset[\"validation\"].select(range(3100, 3200))  # No official SQuAD test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "726043e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of training set: 1000\n",
      "Size of validation set: 100\n",
      "Size of testing set: 100\n"
     ]
    }
   ],
   "source": [
    "print(\"Size of training set:\", len(train_dataset))\n",
    "print(\"Size of validation set:\", len(val_dataset))\n",
    "print(\"Size of testing set:\", len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5bcfc8e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = \"t5-small\"\n",
    "#MODEL_NAME = \"t5-base\"\n",
    "MAX_INPUT_LENGTH = 512\n",
    "MAX_OUTPUT_LENGTH = 128\n",
    "# Load tokenizer and model\n",
    "tokenizer = T5Tokenizer.from_pretrained(MODEL_NAME)\n",
    "model = T5ForConditionalGeneration.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235072c5-f37e-4f2b-8fdd-b1762c2eca6a",
   "metadata": {},
   "source": [
    "# Preprocessing data before put into t-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d3e2652a-32ac-4754-8bb5-3a8260021062",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_question_and_context(question, context):\n",
    "    return f\"question: {question}  context: {context}\"\n",
    "\n",
    "# Obtains the context, question and answer from a given sample.\n",
    "def extract_sample_parts(sample):\n",
    "    context = sample[\"context\"]\n",
    "    question = sample[\"question\"]\n",
    "    answer = sample[\"answers\"]['text'][0]\n",
    "    question_with_context = encode_question_and_context(question, context)\n",
    "    return (question_with_context, question, answer)\n",
    "\n",
    "# Encodes the sample, returning token IDs.\n",
    "def preprocess(sample):\n",
    "    # Extract data from sample.\n",
    "    question_with_context, question, answer = extract_sample_parts(sample)\n",
    "\n",
    "    # Using truncation causes the tokenizer to emit a warning for every sample.\n",
    "    # This will generate a significant amount of messages, and likely crash\n",
    "    # your browser tab. We temporarily disable log messages to work around this.\n",
    "    # See https://github.com/huggingface/transformers/issues/14285\n",
    "    old_level = transformers_logging.get_verbosity()\n",
    "    transformers_logging.set_verbosity_error()\n",
    "    \n",
    "    # Generate tokens for the input.\n",
    "    # We include both the context and the question (first two parameters).\n",
    "    input_tokens = tokenizer(question_with_context, question, padding=\"max_length\",\n",
    "                             truncation=True, max_length=MAX_INPUT_LENGTH)\n",
    "\n",
    "    # Generate tokens for the expected answer. There is no need to include the \n",
    "    output_tokens = tokenizer(answer, padding=\"max_length\", truncation=True,\n",
    "                              max_length=MAX_OUTPUT_LENGTH)\n",
    "\n",
    "    # Restore old logging level, see above.\n",
    "    transformers_logging.set_verbosity(old_level)\n",
    "\n",
    "    # The output of the tokenizer is a map containing {input_ids, attention_mask}.\n",
    "    # For trianing, we need to add the labels (answer/output tokens) to the map.\n",
    "    input_tokens[\"labels\"] = np.array(output_tokens[\"input_ids\"])\n",
    "\n",
    "    return input_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "88504d94-9c04-4af6-965e-742418624148",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the datasets\n",
    "training_set_enc = train_dataset.map(preprocess, batched=False)\n",
    "validation_set_enc = val_dataset.map(preprocess, batched=False)\n",
    "testing_set_enc = test_dataset.map(preprocess, batched=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aea75f9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "    num_rows: 20\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepare 20 data points for qualitative analysis\n",
    "q_data = test_dataset.select(range(20))\n",
    "q_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b3a49a-3a6a-4c6f-9248-6da78d82e78d",
   "metadata": {},
   "source": [
    "# Fine tuning the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5c4381bd-31fc-484d-a958-75fcdbada341",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"input_ids\", \"attention_mask\", \"labels\"]\n",
    "training_set_enc.set_format(type=\"torch\", columns=columns)\n",
    "validation_set_enc.set_format(type=\"torch\", columns=columns)\n",
    "testing_set_enc.set_format(type=\"torch\", columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f682a594-b143-4a60-97dd-bc7770f18faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "output_dir=\"./results\",\n",
    "num_train_epochs=3,\n",
    "per_device_train_batch_size=8,\n",
    "per_device_eval_batch_size=8,\n",
    "eval_strategy=\"epoch\",\n",
    "save_strategy=\"epoch\",\n",
    "learning_rate=2e-5,\n",
    "weight_decay=0.01,\n",
    "save_total_limit=2,\n",
    "logging_dir=\"./logs\",\n",
    "logging_steps=10,\n",
    "load_best_model_at_end=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fc5ec5e4-02ca-4370-a805-efd9f45e5199",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/opt/anaconda3/lib/python3.12/site-packages/transformers/data/data_collator.py:741: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:256.)\n",
      "  batch[\"labels\"] = torch.tensor(batch[\"labels\"], dtype=torch.int64)\n",
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='375' max='375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [375/375 08:55, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.625300</td>\n",
       "      <td>0.200828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.231000</td>\n",
       "      <td>0.183631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.193800</td>\n",
       "      <td>0.169238</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/opt/anaconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=375, training_loss=2.217956089655558, metrics={'train_runtime': 541.0495, 'train_samples_per_second': 5.545, 'train_steps_per_second': 0.693, 'total_flos': 406025404416000.0, 'train_loss': 2.217956089655558, 'epoch': 3.0})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Switch the model to training mode, enabling dropout etc layers.\n",
    "model.train()\n",
    "trainer = Trainer(\n",
    "model=model,\n",
    "args=training_args,\n",
    "train_dataset=training_set_enc,\n",
    "eval_dataset=validation_set_enc,\n",
    "processing_class=tokenizer,\n",
    "data_collator=DataCollatorForSeq2Seq(tokenizer),\n",
    "callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a49affc4-b598-4a71-b769-ad05df61d301",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"t5_pretrained\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e8c5e4-e1e7-404b-9ab7-dc3235149331",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6e8be442-b267-4346-bb46-fb7e2be174a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='138' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [125/125 01:07]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set Loss: 0.17\n",
      "Testing Set Loss: 0.17\n"
     ]
    }
   ],
   "source": [
    "def display_evaluation(setname, results):\n",
    "    print(f\"{setname} Set Loss:\", round(results[\"eval_loss\"], 3))\n",
    "# Switch the model to evaluation mode, disabling dropout etc layers.\n",
    "model.eval()\n",
    "# Evaluate the datasets.\n",
    "display_evaluation(\"Training\", trainer.evaluate(training_set_enc))\n",
    "display_evaluation(\"Testing\", trainer.evaluate(testing_set_enc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e3f3a68f-1963-4309-92ec-f6dc741a75f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import batched\n",
    "# Generates a response for a single input/question.\n",
    "def generate_response(tokenizer, model, question):\n",
    "# Convert the sentences into a list of numeric tokens. We instruct the tokenizer\n",
    "# to return PyTorch tensors (\"pt\") so that we can feed them directly into the model.\n",
    "    tokenized = tokenizer(question, return_tensors=\"pt\", padding=True, truncation=True,max_length=MAX_OUTPUT_LENGTH).to(model.device)\n",
    "    # Generate outputs using the model.\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**tokenized)\n",
    "    # The model outputs a list of numeric tokens. To convert these tokens back to\n",
    "    # sentences, we can use the batch_decode function from the tokenizer.\n",
    "    outputs = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    return outputs\n",
    "# Generates a list of responses from the specified model, optionally including\n",
    "# the context in the prompt. If limit is set, then answers will only be generated\n",
    "# for the first N questions of the dataset.\n",
    "def generate_answers(tokenizer, model, dataset, use_context=True, limit=None):\n",
    "# Subsampling if requested.\n",
    "    if limit is not None:\n",
    "        dataset = dataset.select(range(limit))\n",
    "# Create list of encoded tokens, similarly to how we preprocessed the data for\n",
    "# training. We do this so we can use batch processing to speed up inference.\n",
    "    questions = []\n",
    "    inputs = []\n",
    "    references = []\n",
    "    for sample in dataset:\n",
    "        question_with_context, question, answer = extract_sample_parts(sample)\n",
    "# Only include the context if the caller requested it.\n",
    "        if use_context:\n",
    "            inputs.append(question_with_context)\n",
    "        else:\n",
    "            inputs.append(question)\n",
    "# Include the original question/answer.\n",
    "        questions.append(question)\n",
    "        references.append(answer)\n",
    "# Generate responses for each of the prompts/inputs.\n",
    "# Submitting each question to the model separately would significantly\n",
    "# increase processing time, especially if the model is located on the GPU.\n",
    "# Instead, we group questions together in the same batch size that we used\n",
    "# for training.\n",
    "    outputs = []\n",
    "    for samples in batched(inputs, 128):\n",
    "        # Python's batched() function returns a tuple of the batch\n",
    "        # size, which we have to first convert to a list.\n",
    "        responses = generate_response(tokenizer, model, list(samples))\n",
    "        # generate_responses() returns an equal-sized list of responses.\n",
    "        outputs.extend(responses)\n",
    "    # The length of the reference responses should equal the length of the\n",
    "    # generated responses.\n",
    "    assert (len(outputs) == len(references))\n",
    "    return outputs, references, questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "84fbcae5-afed-41b1-8ba0-753f9c02ba4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers_ctx, refs_ctx, questions_ctx = generate_answers(tokenizer, model, test_dataset, True, 100)\n",
    "answers_noctx, refs_noctx, questions_noctx = generate_answers(tokenizer, model, test_dataset, False, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9b5609dc-0901-41c2-8e75-dcfcde1fc3ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Without context ***\n",
      "Question: What country initially received the largest number of Huguenot refugees?\n",
      "Generated answer: \n",
      "Reference answer: the Dutch Republic\n",
      "\n",
      "*** Without context ***\n",
      "Question: How many refugees emigrated to the Dutch Republic?\n",
      "Generated answer: Wie viele Flüchtlinge migrierten in die Niederlande?\n",
      "Reference answer: an estimated total of 75,000 to 100,000 people\n",
      "\n",
      "*** Without context ***\n",
      "Question: What was the population of the Dutch Republic before this emigration?\n",
      "Generated answer: \n",
      "Reference answer: ca. 2 million\n",
      "\n",
      "*** Without context ***\n",
      "Question: What two areas in the Republic were first to grant rights to the Huguenots?\n",
      "Generated answer: Welche beide Gebiete in der Republik waren die Ersten, die die Huguenots\n",
      "Reference answer: Amsterdam and the area of West Frisia\n",
      "\n",
      "*** Without context ***\n",
      "Question: What declaration predicated the emigration of Huguenot refugees?\n",
      "Generated answer: Quelle déclaration a proclamé l'émigration des Huguenot refugees?\n",
      "Reference answer: the revocation of the Edict of Nantes\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def display_answer_and_references(question, answer, reference):\n",
    "    print(\"*** Without context ***\")\n",
    "    print(\"Question:\", question)\n",
    "    print(\"Generated answer:\", answer)\n",
    "    print(\"Reference answer:\", reference)\n",
    "    print()\n",
    "# for i in range(5):\n",
    "#     display_answer_and_references(questions_ctx[i], answers_ctx[i],refs_ctx[i])\n",
    "# print(\"*** Without context ***\")\n",
    "for i in range(5):\n",
    "    display_answer_and_references(questions_noctx[i],answers_noctx[i], refs_noctx[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3923bb-1774-423a-8333-429af062479d",
   "metadata": {},
   "source": [
    "# Rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4baa6c60-1c7c-41e1-8961-d774c7628810",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computes the average score of a given metric from a list of ROUGE scores.\n",
    "def compute_average_score(scores, metric, key):\n",
    "    total = 0\n",
    "    for i in range(len(scores)):\n",
    "    # Since it's not a map, we have to manually read the attribute.\n",
    "        total += getattr(scores[i][metric], key)\n",
    "    return total / len(scores)\n",
    "    # Computes ROGUE-1, ROGUE-2 and ROGUE-L scores for the given generated\n",
    "    # answers and reference answers.\n",
    "def compute_rouge(predictions, references):\n",
    "    # Compute ROUGE-1, ROGUE-2 and ROUGE-L.\n",
    "    metrics = [\"rouge1\", \"rouge2\", \"rougeL\"]\n",
    "    # Use Porter stemmer to strip word suffixes to improve matching.\n",
    "    scorer = rouge_scorer.RougeScorer(metrics, use_stemmer=True)\n",
    "    # For each answer/reference pair, compute the ROUGE metrics.\n",
    "    scores = []\n",
    "    for prediction, reference in zip(predictions, references):\n",
    "        scores.append(scorer.score(reference, prediction))\n",
    "    # Compute the average precision, recall and F1 score for each metric.\n",
    "    results = {}\n",
    "    for metric in metrics:\n",
    "        for k in [\"precision\", \"recall\", \"fmeasure\"]:\n",
    "            results[f\"{metric}_{k}\"] = compute_average_score(\n",
    "                scores, metric, k)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "530e4eb2-02eb-43fd-8054-c74b67f1b606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE with context: {'rouge1_precision': 0.32556060606060605, 'rouge1_recall': 0.2933791208791209, 'rouge1_fmeasure': 0.29371724431399354, 'rouge2_precision': 0.19666666666666668, 'rouge2_recall': 0.16175, 'rouge2_fmeasure': 0.17185714285714287, 'rougeL_precision': 0.3246515151515152, 'rougeL_recall': 0.29212912087912085, 'rougeL_fmeasure': 0.2926646127350462}\n",
      "\n",
      "ROUGE without context: {'rouge1_precision': 0.0019090909090909091, 'rouge1_recall': 0.0044444444444444444, 'rouge1_fmeasure': 0.0025384615384615385, 'rouge2_precision': 0.0, 'rouge2_recall': 0.0, 'rouge2_fmeasure': 0.0, 'rougeL_precision': 0.0019090909090909091, 'rougeL_recall': 0.0044444444444444444, 'rougeL_fmeasure': 0.0025384615384615385}\n"
     ]
    }
   ],
   "source": [
    "print(\"ROUGE with context:\", compute_rouge(answers_ctx, refs_ctx))\n",
    "print()\n",
    "print(\"ROUGE without context:\", compute_rouge(answers_noctx, refs_noctx))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17fec84-33be-4166-babe-2b0f2da10654",
   "metadata": {},
   "source": [
    "# mrm8488/t5-base-finetuned-squadv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "87645519-692f-42b7-8453-e20a697c617f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<pad> HF-Transformers and Google</s>'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"mrm8488/t5-base-finetuned-squadv2\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"mrm8488/t5-base-finetuned-squadv2\")\n",
    "\n",
    "def get_answer(question, context):\n",
    "  input_text = \"question: %s  context: %s\" % (question, context)\n",
    "  features = tokenizer([input_text], return_tensors='pt')\n",
    "\n",
    "  output = model.generate(input_ids=features['input_ids'], \n",
    "               attention_mask=features['attention_mask'])\n",
    "  \n",
    "  return tokenizer.decode(output[0])\n",
    "\n",
    "context = \"Manuel have created RuPERTa-base with the support of HF-Transformers and Google\"\n",
    "question = \"Who has supported Manuel?\"\n",
    "\n",
    "get_answer(question, context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3c2fecb5-f0e3-4fb5-a29d-9ac9c8bdca3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generates a response for a single input/question.\n",
    "def generate_response(tokenizer, model, question):\n",
    "# Convert the sentences into a list of numeric tokens. We instruct the tokenizer\n",
    "# to return PyTorch tensors (\"pt\") so that we can feed them directly into the model.\n",
    "    tokenized = tokenizer(question, return_tensors=\"pt\", padding=True, truncation=True,max_length=MAX_OUTPUT_LENGTH).to(model.device)\n",
    "    # Generate outputs using the model.\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**tokenized)\n",
    "    # The model outputs a list of numeric tokens. To convert these tokens back to\n",
    "    # sentences, we can use the batch_decode function from the tokenizer.\n",
    "    outputs = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    return outputs\n",
    "# Generates a list of responses from the specified model, optionally including\n",
    "# the context in the prompt. If limit is set, then answers will only be generated\n",
    "# for the first N questions of the dataset.\n",
    "def generate_answers(tokenizer, model, dataset, use_context=True, limit=None):\n",
    "# Subsampling if requested.\n",
    "    if limit is not None:\n",
    "        dataset = dataset.select(range(limit))\n",
    "# Create list of encoded tokens, similarly to how we preprocessed the data for\n",
    "# training. We do this so we can use batch processing to speed up inference.\n",
    "    questions = []\n",
    "    inputs = []\n",
    "    references = []\n",
    "    for sample in dataset:\n",
    "        question_with_context, question, answer = extract_sample_parts(sample)\n",
    "# Only include the context if the caller requested it.\n",
    "        if use_context:\n",
    "            inputs.append(question_with_context)\n",
    "        else:\n",
    "            inputs.append(question)\n",
    "# Include the original question/answer.\n",
    "        questions.append(question)\n",
    "        references.append(answer)\n",
    "# Generate responses for each of the prompts/inputs.\n",
    "# Submitting each question to the model separately would significantly\n",
    "# increase processing time, especially if the model is located on the GPU.\n",
    "# Instead, we group questions together in the same batch size that we used\n",
    "# for training.\n",
    "    outputs = []\n",
    "    for samples in batched(inputs, 128):\n",
    "        # Python's batched() function returns a tuple of the batch\n",
    "        # size, which we have to first convert to a list.\n",
    "        responses = generate_response(tokenizer, model, list(samples))\n",
    "        # generate_responses() returns an equal-sized list of responses.\n",
    "        outputs.extend(responses)\n",
    "    # The length of the reference responses should equal the length of the\n",
    "    # generated responses.\n",
    "    assert (len(outputs) == len(references))\n",
    "    return outputs, references, questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6854bcb7-d533-417b-be3f-51141bf2733d",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers_ctx, refs_ctx, questions_ctx = generate_answers(tokenizer, model, test_dataset, True, 100)\n",
    "answers_noctx, refs_noctx, questions_noctx = generate_answers(tokenizer, model, test_dataset, False, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8787a718-2f83-402a-8050-61a1ac968087",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Without context ***\n",
      "Question: What country initially received the largest number of Huguenot refugees?\n",
      "Generated answer: Lieu initial de réfugié(n0,n1)|\n",
      "Reference answer: the Dutch Republic\n",
      "\n",
      "*** Without context ***\n",
      "Question: How many refugees emigrated to the Dutch Republic?\n",
      "Generated answer: divide(n0,const_100)|divide(#0,const_\n",
      "Reference answer: an estimated total of 75,000 to 100,000 people\n",
      "\n",
      "*** Without context ***\n",
      "Question: What was the population of the Dutch Republic before this emigration?\n",
      "Generated answer: add(const_1,const_4)|population_year(n0)|\n",
      "Reference answer: ca. 2 million\n",
      "\n",
      "*** Without context ***\n",
      "Question: What two areas in the Republic were first to grant rights to the Huguenots?\n",
      "Generated answer: \n",
      "Reference answer: Amsterdam and the area of West Frisia\n",
      "\n",
      "*** Without context ***\n",
      "Question: What declaration predicated the emigration of Huguenot refugees?\n",
      "Generated answer:          \n",
      "Reference answer: the revocation of the Edict of Nantes\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def display_answer_and_references(question, answer, reference):\n",
    "    print(\"*** Without context ***\")\n",
    "    print(\"Question:\", question)\n",
    "    print(\"Generated answer:\", answer)\n",
    "    print(\"Reference answer:\", reference)\n",
    "    print()\n",
    "# for i in range(5):\n",
    "#     display_answer_and_references(questions_ctx[i], answers_ctx[i],refs_ctx[i])\n",
    "# print(\"*** Without context ***\")\n",
    "for i in range(5):\n",
    "    display_answer_and_references(questions_noctx[i],answers_noctx[i], refs_noctx[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b3b0ab7c-c86f-4639-92e3-86f828afbe2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computes the average score of a given metric from a list of ROUGE scores.\n",
    "def compute_average_score(scores, metric, key):\n",
    "    total = 0\n",
    "    for i in range(len(scores)):\n",
    "    # Since it's not a map, we have to manually read the attribute.\n",
    "        total += getattr(scores[i][metric], key)\n",
    "    return total / len(scores)\n",
    "    # Computes ROGUE-1, ROGUE-2 and ROGUE-L scores for the given generated\n",
    "    # answers and reference answers.\n",
    "def compute_rouge(predictions, references):\n",
    "    # Compute ROUGE-1, ROGUE-2 and ROUGE-L.\n",
    "    metrics = [\"rouge1\", \"rouge2\", \"rougeL\"]\n",
    "    # Use Porter stemmer to strip word suffixes to improve matching.\n",
    "    scorer = rouge_scorer.RougeScorer(metrics, use_stemmer=True)\n",
    "    # For each answer/reference pair, compute the ROUGE metrics.\n",
    "    scores = []\n",
    "    for prediction, reference in zip(predictions, references):\n",
    "        scores.append(scorer.score(reference, prediction))\n",
    "    # Compute the average precision, recall and F1 score for each metric.\n",
    "    results = {}\n",
    "    for metric in metrics:\n",
    "        for k in [\"precision\", \"recall\", \"fmeasure\"]:\n",
    "            results[f\"{metric}_{k}\"] = compute_average_score(\n",
    "                scores, metric, k)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c2a7e1c0-1812-4729-a537-333de031897f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE with context: {'rouge1_precision': 0.5609801587301587, 'rouge1_recall': 0.5676111111111111, 'rouge1_fmeasure': 0.5423138626079801, 'rouge2_precision': 0.3511666666666666, 'rouge2_recall': 0.35424999999999995, 'rouge2_fmeasure': 0.3412698412698413, 'rougeL_precision': 0.5609801587301587, 'rougeL_recall': 0.5676111111111111, 'rougeL_fmeasure': 0.5423138626079801}\n",
      "\n",
      "ROUGE without context: {'rouge1_precision': 0.0057619047619047615, 'rouge1_recall': 0.0037777777777777775, 'rouge1_fmeasure': 0.004451324389404885, 'rouge2_precision': 0.0, 'rouge2_recall': 0.0, 'rouge2_fmeasure': 0.0, 'rougeL_precision': 0.0057619047619047615, 'rougeL_recall': 0.0037777777777777775, 'rougeL_fmeasure': 0.004451324389404885}\n"
     ]
    }
   ],
   "source": [
    "print(\"ROUGE with context:\", compute_rouge(answers_ctx, refs_ctx))\n",
    "print()\n",
    "print(\"ROUGE without context:\", compute_rouge(answers_noctx, refs_noctx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9f1bed-c789-4ab9-89e4-dd93b6f5fa37",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7fc7f33-94fe-4ab8-94ff-27347c335568",
   "metadata": {},
   "source": [
    "# Prepare SQuAD_tiny Dataset for Assignment 2\n",
    "\n",
    "This code prepare SQuAD_tiny from the SQuAD dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00cc501e-66e7-4dd8-9ede-7dd516c57230",
   "metadata": {},
   "source": [
    "# 0. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "debe47fc-6d7a-4fcb-b07f-7435895e88ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments, DataCollatorForSeq2Seq, EarlyStoppingCallback, TrainerCallback\n",
    "from rouge_score import rouge_scorer\n",
    "from tqdm import tqdm\n",
    "from transformers import logging as transformers_logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "28c25606-856c-4e9b-bbb3-86d0abdd798b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1464baab0>"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set seed for reproducibility\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f9a46e-5f00-41af-9148-a6eefb6d08ed",
   "metadata": {},
   "source": [
    "# 1. Load and preprocess SQuAD dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "c0437040-ad9b-4cfe-9801-d1a801a7102e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load and preprocess SQuAD dataset\n",
    "dataset = load_dataset(\"squad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "42bd720f-173b-47ea-9c85-abe55b0ae8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take subsets to avoid overload\n",
    "train_dataset = dataset[\"train\"].select(range(10000,11000))\n",
    "val_dataset = dataset[\"validation\"].select(range(3000,3100))\n",
    "test_dataset = dataset[\"validation\"].select(range(3100, 3200))  # No official SQuAD test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "726043e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of training set: 1000\n",
      "Size of validation set: 100\n",
      "Size of testing set: 100\n"
     ]
    }
   ],
   "source": [
    "print(\"Size of training set:\", len(train_dataset))\n",
    "print(\"Size of validation set:\", len(val_dataset))\n",
    "print(\"Size of testing set:\", len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "5bcfc8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"t5-small\"\n",
    "#MODEL_NAME = \"t5-base\"\n",
    "MAX_INPUT_LENGTH = 512\n",
    "MAX_OUTPUT_LENGTH = 128\n",
    "# Load tokenizer and model\n",
    "tokenizer = T5Tokenizer.from_pretrained(MODEL_NAME)\n",
    "model = T5ForConditionalGeneration.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235072c5-f37e-4f2b-8fdd-b1762c2eca6a",
   "metadata": {},
   "source": [
    "# Preprocessing data before put into t-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "d3e2652a-32ac-4754-8bb5-3a8260021062",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_question_and_context(question, context):\n",
    "    return f\"question: {question}  context: {context}\"\n",
    "\n",
    "# Obtains the context, question and answer from a given sample.\n",
    "def extract_sample_parts(sample):\n",
    "    context = sample[\"context\"]\n",
    "    question = sample[\"question\"]\n",
    "    answer = sample[\"answers\"]['text'][0]\n",
    "    question_with_context = encode_question_and_context(question, context)\n",
    "    return (question_with_context, question, answer)\n",
    "\n",
    "# Encodes the sample, returning token IDs.\n",
    "def preprocess(sample):\n",
    "    # Extract data from sample.\n",
    "    question_with_context, question, answer = extract_sample_parts(sample)\n",
    "\n",
    "    # Using truncation causes the tokenizer to emit a warning for every sample.\n",
    "    # This will generate a significant amount of messages, and likely crash\n",
    "    # your browser tab. We temporarily disable log messages to work around this.\n",
    "    # See https://github.com/huggingface/transformers/issues/14285\n",
    "    old_level = transformers_logging.get_verbosity()\n",
    "    transformers_logging.set_verbosity_error()\n",
    "    \n",
    "    # Generate tokens for the input.\n",
    "    # We include both the context and the question (first two parameters).\n",
    "    input_tokens = tokenizer(question_with_context, question, padding=\"max_length\",\n",
    "                             truncation=True, max_length=MAX_INPUT_LENGTH)\n",
    "\n",
    "    # Generate tokens for the expected answer. There is no need to include the \n",
    "    output_tokens = tokenizer(answer, padding=\"max_length\", truncation=True,\n",
    "                              max_length=MAX_OUTPUT_LENGTH)\n",
    "\n",
    "    # Restore old logging level, see above.\n",
    "    transformers_logging.set_verbosity(old_level)\n",
    "\n",
    "    # The output of the tokenizer is a map containing {input_ids, attention_mask}.\n",
    "    # For trianing, we need to add the labels (answer/output tokens) to the map.\n",
    "    input_tokens[\"labels\"] = np.array(output_tokens[\"input_ids\"])\n",
    "\n",
    "    return input_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "88504d94-9c04-4af6-965e-742418624148",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the datasets\n",
    "training_set_enc = train_dataset.map(preprocess, batched=False)\n",
    "validation_set_enc = val_dataset.map(preprocess, batched=False)\n",
    "testing_set_enc = test_dataset.map(preprocess, batched=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "aea75f9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "    num_rows: 20\n",
       "})"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepare 20 data points for qualitative analysis\n",
    "q_data = test_dataset.select(range(20))\n",
    "q_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b3a49a-3a6a-4c6f-9248-6da78d82e78d",
   "metadata": {},
   "source": [
    "# Fine tuning the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "5c4381bd-31fc-484d-a958-75fcdbada341",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"input_ids\", \"attention_mask\", \"labels\"]\n",
    "training_set_enc.set_format(type=\"torch\", columns=columns)\n",
    "validation_set_enc.set_format(type=\"torch\", columns=columns)\n",
    "testing_set_enc.set_format(type=\"torch\", columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "f682a594-b143-4a60-97dd-bc7770f18faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "output_dir=\"./results\",\n",
    "num_train_epochs=3,\n",
    "per_device_train_batch_size=8,\n",
    "per_device_eval_batch_size=8,\n",
    "eval_strategy=\"epoch\",\n",
    "save_strategy=\"epoch\",\n",
    "learning_rate=3e-4,\n",
    "weight_decay=0.01,\n",
    "save_total_limit=2,\n",
    "logging_dir=\"./logs\",\n",
    "logging_steps=10,\n",
    "load_best_model_at_end=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "fc5ec5e4-02ca-4370-a805-efd9f45e5199",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='375' max='375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [375/375 08:05, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.031500</td>\n",
       "      <td>0.021843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.020300</td>\n",
       "      <td>0.020282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.014600</td>\n",
       "      <td>0.020615</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/opt/anaconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=375, training_loss=0.2295675111611684, metrics={'train_runtime': 487.3959, 'train_samples_per_second': 6.155, 'train_steps_per_second': 0.769, 'total_flos': 406025404416000.0, 'train_loss': 0.2295675111611684, 'epoch': 3.0})"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Switch the model to training mode, enabling dropout etc layers.\n",
    "model.train()\n",
    "trainer = Trainer(\n",
    "model=model,\n",
    "args=training_args,\n",
    "train_dataset=training_set_enc,\n",
    "eval_dataset=validation_set_enc,\n",
    "processing_class=tokenizer,\n",
    "data_collator=DataCollatorForSeq2Seq(tokenizer),\n",
    "callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "a49affc4-b598-4a71-b769-ad05df61d301",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"t5_pretrained\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e8c5e4-e1e7-404b-9ab7-dc3235149331",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "6e8be442-b267-4346-bb46-fb7e2be174a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='138' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [125/125 00:34]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set Loss: 0.009\n",
      "Testing Set Loss: 0.019\n"
     ]
    }
   ],
   "source": [
    "def display_evaluation(setname, results):\n",
    "    print(f\"{setname} Set Loss:\", round(results[\"eval_loss\"], 3))\n",
    "# Switch the model to evaluation mode, disabling dropout etc layers.\n",
    "model.eval()\n",
    "# Evaluate the datasets.\n",
    "display_evaluation(\"Training\", trainer.evaluate(training_set_enc))\n",
    "display_evaluation(\"Testing\", trainer.evaluate(testing_set_enc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f3a68f-1963-4309-92ec-f6dc741a75f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import batched\n",
    "# Generates a response for a single input/question.\n",
    "def generate_response(tokenizer, model, question):\n",
    "# Convert the sentences into a list of numeric tokens. We instruct the tokenizer\n",
    "# to return PyTorch tensors (\"pt\") so that we can feed them directly into the model.\n",
    "    tokenized = tokenizer(question, return_tensors=\"pt\", padding=True, truncation=True,max_length=MAX_OUTPUT_LENGTH).to(model.device)\n",
    "    # Generate outputs using the model.\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**tokenized)\n",
    "    # The model outputs a list of numeric tokens. To convert these tokens back to\n",
    "    # sentences, we can use the batch_decode function from the tokenizer.\n",
    "    outputs = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    return outputs\n",
    "# Generates a list of responses from the specified model, optionally including\n",
    "# the context in the prompt. If limit is set, then answers will only be generated\n",
    "# for the first N questions of the dataset.\n",
    "def generate_answers(tokenizer, model, dataset, use_context=True, limit=None):\n",
    "# Subsampling if requested.\n",
    "    if limit is not None:\n",
    "        dataset = dataset.select(range(limit))\n",
    "# Create list of encoded tokens, similarly to how we preprocessed the data for\n",
    "# training. We do this so we can use batch processing to speed up inference.\n",
    "    questions = []\n",
    "    inputs = []\n",
    "    references = []\n",
    "    for sample in dataset:\n",
    "        question_with_context, question, answer = extract_sample_parts(sample)\n",
    "# Only include the context if the caller requested it.\n",
    "        if use_context:\n",
    "            inputs.append(question_with_context)\n",
    "        else:\n",
    "            inputs.append(question)\n",
    "# Include the original question/answer.\n",
    "        questions.append(question)\n",
    "        references.append(answer)\n",
    "# Generate responses for each of the prompts/inputs.\n",
    "# Submitting each question to the model separately would significantly\n",
    "# increase processing time, especially if the model is located on the GPU.\n",
    "# Instead, we group questions together in the same batch size that we used\n",
    "# for training.\n",
    "    outputs = []\n",
    "    for samples in batched(inputs, 128):\n",
    "        # Python's batched() function returns a tuple of the batch\n",
    "        # size, which we have to first convert to a list.\n",
    "        responses = generate_response(tokenizer, model, list(samples))\n",
    "        # generate_responses() returns an equal-sized list of responses.\n",
    "        outputs.extend(responses)\n",
    "    # The length of the reference responses should equal the length of the\n",
    "    # generated responses.\n",
    "    assert (len(outputs) == len(references))\n",
    "    return outputs, references, questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84fbcae5-afed-41b1-8ba0-753f9c02ba4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers_ctx, refs_ctx, questions_ctx = generate_answers(tokenizer, model, testing_set, True, 100)\n",
    "answers_noctx, refs_noctx, questions_noctx = generate_answers(tokenizer, model, testing_set, False, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5609dc-0901-41c2-8e75-dcfcde1fc3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_answer_and_references(question, answer, reference):\n",
    "    print(\"Question:\", question)\n",
    "    print(\"Generated answer:\", answer)\n",
    "    print(\"Reference answer:\", reference)\n",
    "    print()\n",
    "    print(\"*** With context ***\")\n",
    "for i in range(3):\n",
    "    display_answer_and_references(questions_ctx[i], answers_ctx[i],refs_ctx[i])\n",
    "print(\"*** Without context ***\")\n",
    "for i in range(3):\n",
    "    display_answer_and_references(questions_noctx[i],answers_noctx[i], refs_noctx[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3923bb-1774-423a-8333-429af062479d",
   "metadata": {},
   "source": [
    "# Rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "4baa6c60-1c7c-41e1-8961-d774c7628810",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computes the average score of a given metric from a list of ROUGE scores.\n",
    "def compute_average_score(scores, metric, key):\n",
    "    total = 0\n",
    "    for i in range(len(scores)):\n",
    "    # Since it's not a map, we have to manually read the attribute.\n",
    "        total += getattr(scores[i][metric], key)\n",
    "    return total / len(scores)\n",
    "    # Computes ROGUE-1, ROGUE-2 and ROGUE-L scores for the given generated\n",
    "    # answers and reference answers.\n",
    "def compute_rouge(predictions, references):\n",
    "    # Compute ROUGE-1, ROGUE-2 and ROUGE-L.\n",
    "    metrics = [\"rouge1\", \"rouge2\", \"rougeL\"]\n",
    "    # Use Porter stemmer to strip word suffixes to improve matching.\n",
    "    scorer = rouge_scorer.RougeScorer(metrics, use_stemmer=True)\n",
    "    # For each answer/reference pair, compute the ROUGE metrics.\n",
    "    scores = []\n",
    "    for prediction, reference in zip(predictions, references):\n",
    "        scores.append(scorer.score(reference, prediction))\n",
    "    # Compute the average precision, recall and F1 score for each metric.\n",
    "    results = {}\n",
    "    for metric in metrics:\n",
    "        for k in [\"precision\", \"recall\", \"fmeasure\"]:\n",
    "            results[f\"{metric}_{k}\"] = compute_average_score(\n",
    "                scores, metric, k)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "530e4eb2-02eb-43fd-8054-c74b67f1b606",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'answers_ctx' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[155], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mROUGE with context:\u001b[39m\u001b[38;5;124m\"\u001b[39m, compute_rouge(answers_ctx, refs_ctx))\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m()\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mROUGE without context:\u001b[39m\u001b[38;5;124m\"\u001b[39m, compute_rouge(answers_noctx, refs_noctx))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'answers_ctx' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"ROUGE with context:\", compute_rouge(answers_ctx, refs_ctx))\n",
    "print()\n",
    "print(\"ROUGE without context:\", compute_rouge(answers_noctx, refs_noctx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87645519-692f-42b7-8453-e20a697c617f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

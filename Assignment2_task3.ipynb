{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2c6bedc7-d296-4997-9394-a47e3cd24549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3691 entries, 0 to 3690\n",
      "Data columns (total 10 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   PADDOCK_ID          3691 non-null   object \n",
      " 1   OBSERVATION_DATE    3691 non-null   object \n",
      " 2   TSDM                3691 non-null   float64\n",
      " 3   15D_AVG_DAILY_RAIN  3691 non-null   float64\n",
      " 4   15D_AVG_MAX_TEMP    3691 non-null   float64\n",
      " 5   15D_AVG_MIN_TEMP    3691 non-null   float64\n",
      " 6   15D_AVG_RH_TMAX     3691 non-null   float64\n",
      " 7   15D_AVG_RH_TMIN     3691 non-null   float64\n",
      " 8   15D_AVG_EVAP_SYN    3691 non-null   float64\n",
      " 9   15D_AVG_RADIATION   3691 non-null   float64\n",
      "dtypes: float64(8), object(2)\n",
      "memory usage: 288.5+ KB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# read the AggriWebb dataset\n",
    "df = pd.read_csv('tsdm.csv')\n",
    "\n",
    "# get info of all \n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "919a145e-0bc8-4d49-9969-cab08e954d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by paddock and date\n",
    "df = df.sort_values(['PADDOCK_ID', 'OBSERVATION_DATE'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "667e24fc-eb78-4704-a021-de449c3c2098",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['PADDOCK_ID'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b69beb7c-d10c-4bf0-9991-3017afd1262e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PADDOCK_ID\n",
       "352a2a92802487c32fa051f18f0fd7c53ef4b517d562a78369c7f8c679c6ac60    195\n",
       "ab55bcd82ea3bedbf39af4eaf9b2c6387ce1d5345fa552d25a7357c10cb689ba    195\n",
       "e59f403db9fb2286ae2a0838ccdc0bcf703b1cf04b933261f4ead66c3d65dda8    195\n",
       "deb009be1806257c59d7964da7bc06206fe74b94d01d308939079b3d57141a26    195\n",
       "d1a2be8f6307b3c2d1252c096b5efcaf7059b3e6ded59c01ea235451af0556e7    195\n",
       "cabef0e85bd2e0922159406f5ad08e17006d466c83d785d06fa7f2c8555acc44    195\n",
       "44f7088d1e8d42e054cd25c74d4bafee2059788c5ac8273a41af5d197c187ff8    195\n",
       "ad87a9f82ee7fb99ddfd4f63e8065925e8411b9e904cd6d4729de5cdee217fa8    195\n",
       "a649402b3679b6bc9ef3556484c74b3b212db6e121e11d14b530c877411de9a9    195\n",
       "8714b98a7839f7e8e139a79e8dbff0ced6b5f68c476d58cfa80c4fee1b02f084    195\n",
       "8191f02814bdec2922fd5287d8e55410e0fd78e2b195b47bdb05830ad71cb0cb    195\n",
       "f4bfb0920671ab818ebac30a7945b3a1ae8e75954ae03fb6f5fef97822ad0883    195\n",
       "b993487b23eecfd37c2c8de5b2816bb7b8d0ff15c5bc0b572533a5a6bd7bf4ae    193\n",
       "6b294fede7561d76c0b2592b9cfc8c131e8400860df9edc319981adb1afd1e0b    193\n",
       "d54f90a4aba145794a09808f912b5c02a413b054d26a3b375db806463d1854cb    193\n",
       "d948ac9f3cded5997708a858bdd25209d1e3b884a9e27698827fb08f624d574a    193\n",
       "5c4c28a9b5e43c10c32f7e2d905672fe9f9225b6ff9c22d3f228de17dfec4008    193\n",
       "56f79406af9de994603912c9f3a883d19502cdcd26840529370b7eb3f0044fee    193\n",
       "ec69095b93c4aae8d9142122118cbab81e7cfd2f5c30f140d1f15540113ad426    193\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['PADDOCK_ID'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d45fe16e-56e1-4a44-b3ee-276979cd8f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['PADDOCK_ID'] != 'b993487b23eecfd37c2c8de5b2816bb7b8d0ff15c5bc0b572533a5a6bd7bf4ae']\n",
    "df = df[df['PADDOCK_ID'] != '6b294fede7561d76c0b2592b9cfc8c131e8400860df9edc319981adb1afd1e0b']\n",
    "df = df[df['PADDOCK_ID'] != 'd54f90a4aba145794a09808f912b5c02a413b054d26a3b375db806463d1854cb']\n",
    "df = df[df['PADDOCK_ID'] != 'd948ac9f3cded5997708a858bdd25209d1e3b884a9e27698827fb08f624d574a']\n",
    "df = df[df['PADDOCK_ID'] != '5c4c28a9b5e43c10c32f7e2d905672fe9f9225b6ff9c22d3f228de17dfec4008']\n",
    "df = df[df['PADDOCK_ID'] != '56f79406af9de994603912c9f3a883d19502cdcd26840529370b7eb3f0044fee']\n",
    "df = df[df['PADDOCK_ID'] != 'ec69095b93c4aae8d9142122118cbab81e7cfd2f5c30f140d1f15540113ad426']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2824cbb3-8b97-415c-8f2a-5b51a07bab46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PADDOCK_ID\n",
       "352a2a92802487c32fa051f18f0fd7c53ef4b517d562a78369c7f8c679c6ac60    195\n",
       "44f7088d1e8d42e054cd25c74d4bafee2059788c5ac8273a41af5d197c187ff8    195\n",
       "8191f02814bdec2922fd5287d8e55410e0fd78e2b195b47bdb05830ad71cb0cb    195\n",
       "8714b98a7839f7e8e139a79e8dbff0ced6b5f68c476d58cfa80c4fee1b02f084    195\n",
       "a649402b3679b6bc9ef3556484c74b3b212db6e121e11d14b530c877411de9a9    195\n",
       "ab55bcd82ea3bedbf39af4eaf9b2c6387ce1d5345fa552d25a7357c10cb689ba    195\n",
       "ad87a9f82ee7fb99ddfd4f63e8065925e8411b9e904cd6d4729de5cdee217fa8    195\n",
       "cabef0e85bd2e0922159406f5ad08e17006d466c83d785d06fa7f2c8555acc44    195\n",
       "d1a2be8f6307b3c2d1252c096b5efcaf7059b3e6ded59c01ea235451af0556e7    195\n",
       "deb009be1806257c59d7964da7bc06206fe74b94d01d308939079b3d57141a26    195\n",
       "e59f403db9fb2286ae2a0838ccdc0bcf703b1cf04b933261f4ead66c3d65dda8    195\n",
       "f4bfb0920671ab818ebac30a7945b3a1ae8e75954ae03fb6f5fef97822ad0883    195\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['PADDOCK_ID'].nunique()\n",
    "df['PADDOCK_ID'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "56e44f65-3db6-4af9-8da6-b664be4335f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sort_values(by=['PADDOCK_ID', 'OBSERVATION_DATE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "86dc7df6-fffd-4485-8ac4-5f6642992fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('preprocessed.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a9e0e7ec-9a05-4fc3-bf00-b274d79eb153",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (2280, 9)\n",
      "Test shape: (60, 9)\n"
     ]
    }
   ],
   "source": [
    "# Assuming df is already sorted by PADDOCK_ID and OBSERVATION_DATE\n",
    "X_train_list, X_test_list, y_train_list, y_test_list = [], [], [], []\n",
    "\n",
    "# Loop through each paddock group\n",
    "for pid, group in df.groupby('PADDOCK_ID'):\n",
    "    # Ensure it's sorted by date\n",
    "    group = group.sort_values('OBSERVATION_DATE')\n",
    "    \n",
    "    # Separate features (drop PADDOCK_ID)\n",
    "    features = group.drop(['PADDOCK_ID'], axis=1)\n",
    "    \n",
    "    # Split into train/test: last 5 timesteps â†’ test\n",
    "    X_train = features.iloc[:-5]\n",
    "    X_test  = features.iloc[-5:]\n",
    "    \n",
    "    # Store\n",
    "    X_train_list.append(X_train)\n",
    "    X_test_list.append(X_test)\n",
    "    \n",
    "    # If you want to store the paddock IDs (labels)\n",
    "    y_train_list.append([pid] * len(X_train))\n",
    "    y_test_list.append([pid] * len(X_test))\n",
    "\n",
    "# Combine into single DataFrames if needed\n",
    "X_train = pd.concat(X_train_list)\n",
    "X_test  = pd.concat(X_test_list)\n",
    "y_train = pd.Series([pid for sublist in y_train_list for pid in sublist], name='PADDOCK_ID')\n",
    "y_test  = pd.Series([pid for sublist in y_test_list for pid in sublist], name='PADDOCK_ID')\n",
    "\n",
    "print(\"Train shape:\", X_train.shape)\n",
    "print(\"Test shape:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "86a36bd7-3828-48d9-bcf6-e7fee087b502",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(sequence, lookback, forecast_horizon, target_col):\n",
    "    T, num_features = sequence.shape\n",
    "    X, y, lengths = [], [], []\n",
    "    pad_vector = np.zeros((lookback, num_features))\n",
    "    # Fixed-length lookback with pre-padding\n",
    "    for t in range(1, T - forecast_horizon + 1):\n",
    "        context = sequence[:t]\n",
    "    if len(context) > lookback:\n",
    "        context = context[-lookback:]\n",
    "        padded_context = pad_vector.copy()\n",
    "        padded_context[-len(context):] = context\n",
    "        X.append(padded_context)\n",
    "        y.append(sequence[t:t + forecast_horizon, target_col])\n",
    "        lengths.append(min(len(context), lookback))\n",
    "    return np.array(X), np.array(y), lengths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4b632ea2-6a89-45cd-ac2c-df0374c292e9",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MinMaxScaler\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdata_prep\u001b[39m(df, feature_columns, lookback, test_steps, target_col):\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m# Prepare to store all training data\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "def data_prep(df, feature_columns, lookback, test_steps, target_col):\n",
    "    # Prepare to store all training data\n",
    "    X_all, y_all = [], []\n",
    "    location_ids = [] # To track which location each sample comes from\n",
    "    test_data = [] # To store test data for each location\n",
    "    train_data = []\n",
    "    lengths_all = [] # To store the lengths of each sequence for packing\n",
    "    # Fit a global scaler\n",
    "    all_train_values = []\n",
    "    for _, group in df.groupby(\"Location\"):\n",
    "        feature_values = group[feature_columns].values\n",
    "        if len(feature_values) > lookback + test_steps:\n",
    "            all_train_values.append(feature_values[:-test_steps])\n",
    "        all_train_values = np.vstack(all_train_values)\n",
    "        global_scaler = MinMaxScaler()\n",
    "        global_scaler.fit(all_train_values)\n",
    "        # Process each location's data\n",
    "        for location_id, group in df.groupby('Location'):\n",
    "            feature_values = group[feature_columns].values\n",
    "        # If there are not enough samples for training, skip this location\n",
    "        if len(feature_values) <= 63:\n",
    "            continue\n",
    "        # Separate the last 12 steps for testing\n",
    "        # Scale the data\n",
    "        train_sample = global_scaler.transform(feature_values[:-test_steps])\n",
    "        test_sample = global_scaler.transform(feature_values[-test_steps:])\n",
    "        # If you prefer without scaling\n",
    "        #train_sample = feature_values[:-test_steps]\n",
    "        #test_sample = feature_values[-test_steps:]\n",
    "        train_data.append((location_id, train_sample))\n",
    "        test_data.append((location_id, test_sample, global_scaler)) # Store tes\n",
    "        # Prepare LSTM sequence data for training\n",
    "        X_location, y_location, lengths = create_sequences(train_sample, lookback)\n",
    "    # Append to the overall dataset\n",
    "    X_all.append(X_location)\n",
    "    y_all.append(y_location)\n",
    "    lengths_all.append(lengths) # Store sequence lengths\n",
    "    # Store Location ID for tracking\n",
    "    location_ids.extend([location_id] * len(y_location))\n",
    "    # Concatenate all locations' training data for model training\n",
    "    X_all = np.concatenate(X_all, axis=0)\n",
    "    y_all = np.concatenate(y_all, axis=0)\n",
    "    lengths_all = np.concatenate(lengths_all, axis=0) # Concatenate sequence le\n",
    "    # Reshape X to be [samples, time steps, features] as required by LSTM\n",
    "    X_all = X_all.reshape((X_all.shape[0], X_all.shape[1], X_all.shape[2]))\n",
    "    return torch.Tensor(X_all), torch.Tensor(y_all), torch.Tensor(lengths_all),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8b1779-8d60-4953-835b-9b6d1c39a014",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
